Overall, I'm using the below 2 variables:
- M: total number of articles in all selected feed
- N: average number of words in each article

1. parseFeed(): O(n * m) runtime, since for each article, each word is looped through to remove special characters and add back to map. With hash map, each look up and addition operation takes O(1) time.

2. buildIndex(): O(n * m) runtime, since the algorithm: first, each word in each article is looped through to calculate word frequency and number of documents that contain each word; afterwards, loop through each word to calculate the TFIDF value. With hash map, each look up and addition operation takes O(1) time.

3. buildInvertedIndex(): O(n * m * log (n * m)) runtime, since the algorithm loops through each word in each article to generate the inverted index which puts the word itself as the key and a map of k-v pairs with all the documents containing the word with its TFIDF value as value. In the process, lookup and get take O(1) time for each word check since it uses hash map data structure. And adding a k-v pair to the treeset takes log(number of elements in truest) time since treeset is a self-balancing BST. Overall, n * m k-v pair entries are added (in the worst case where all words are distinct and no word is repeated) which means the overall runtime is O(n * m * log (n * m)).

4. buildHomePage(): O(n * m * log(n * m)) runtime, since the algorithm loops through each word in each article to generate the word list for homepage. In the process, removing stop words take O(1) time for each word check since it uses hash set data structure. And adding a word pair to the treeset takes log(number of elements in truest) time since treeset is a self-balancing BST. Overall, n * m words are added (in the worst case where all words are distinct and no word is repeated) which means the overall runtime is O(n * m * log (n * m)).

5. createAutocompleteFile(): O(n * m * log (n * m)) runtime, since the algorithm loops through each word in each article to generate the collection for autocomplete file. And then the collection is sorted using Collections.sort() which takes n * m * log(n * m) time to do. Overall, the run time is still O(n * m * log(n * m)). 

6. searchArticles(): O(m) runtime, since it takes O(1) time to find the corresponding entry set to the query term using hash map. Worst case is that the set contains all articles. For each article, it needs to be added to the result. So overall runtime is O(m).